{
	"name": "ollama",
	"version": "1.0.0",
	"description": "Ollama integration for local LLM inference",
	"config": {
		"host": "http://localhost:11434",
		"model": "mistral",
		"parameters": {
			"temperature": 0.7,
			"top_p": 0.9,
			"top_k": 40,
			"repeat_penalty": 1.1
		}
	}
}

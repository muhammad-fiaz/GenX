[
	{
		"id": "ollama",
		"name": "Ollama",
		"description": "Local AI model running through Ollama",
		"enabled": true,
		"config": {
			"baseUrl": "http://localhost:11434",
			"models": ["llama2", "codellama", "mistral", "mixtral"],
			"defaultModel": "llama2",
			"endpoints": {
				"chat": "/api/chat",
				"generate": "/api/generate"
			}
		},
		"parameters": {
			"temperature": {
				"type": "number",
				"default": 0.7,
				"min": 0,
				"max": 1
			},
			"maxTokens": {
				"type": "number",
				"default": 2048,
				"min": 1,
				"max": 4096
			}
		}
	}
]
